{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Copyright 2020 Novartis Institutes for BioMedical Research Inc.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "This file incorporates work covered by the following copyright and  \n",
    "permission notice:\n",
    "\n",
    "   Copyright (c) 2017-present, Facebook, Inc.\n",
    "   All rights reserved.\n",
    "\n",
    "   This source code is licensed as Creative Commons Attribution-Noncommercial \n",
    "   and can be found under https://creativecommons.org/licenses/by-nc/4.0/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMM Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HTTPS_PROXY=http://proxy-server.bms.com:8080\n"
     ]
    }
   ],
   "source": [
    "%env HTTPS_PROXY = http://proxy-server.bms.com:8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU device\n",
    "import os\n",
    "print(os.getenv('CUDA_VISIBLE_DEVICES'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numba.errors import NumbaDeprecationWarning, NumbaPendingDeprecationWarning, NumbaPerformanceWarning\n",
    "# import warnings\n",
    "\n",
    "# warnings.filterwarnings(action='once')\n",
    "# warnings.simplefilter('ignore', category=NumbaDeprecationWarning)\n",
    "# warnings.simplefilter('ignore', category=NumbaPendingDeprecationWarning)\n",
    "# warnings.simplefilter('ignore', category=NumbaPerformanceWarning)\n",
    "# warnings.simplefilter(\"ignore\", category=PendingDeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torchsummary import summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import clustering\n",
    "from model import MultiScaleNet\n",
    "from util import Logger, UnifLabelSampler\n",
    "from training import compute_features, train\n",
    "# from correction import do_batch_correction\n",
    "# from evaluation import evaluate_epoch, evaluate_training\n",
    "from my_dataset import bbbc021_dataset\n",
    "# from plot import plot_training, plot_training_summary, plot_inter_part_validation, plot_inter_hier_validation, plot_external_validation, plot_NSC_NSB, plot_total_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = bbbc021_dataset(args.data, args.csv, 'pseudoclass', transform=transforms.Compose(tra))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    # Experiment ID\n",
    "    run_ID = \"EXP01\"\n",
    "    \n",
    "    # path to dataset\n",
    "    data = os.getcwd()#\"/path/to/dataset\"\n",
    "    \n",
    "    # file name of the metadata csv file\n",
    "    # csv = \"dataset.csv\"\n",
    "    csv = \"sample_data.csv\"\n",
    "    \n",
    "    # path to the output folder\n",
    "    save_path = \"/path/to/output/folder/\"\n",
    "    output_path = \"\" #dont assign\n",
    "\n",
    "    # CNN architecture \n",
    "    arch = \"msnet\" \n",
    "\n",
    "    # input channels\n",
    "    n_input_dim = 3\n",
    "    \n",
    "    # number of features\n",
    "    n_features = 64\n",
    "    \n",
    "    # Batch correction [\"TVN\", \"COMBAT\", MNN]\n",
    "    batch_corrections = [\"COMBAT\", \"TVN\"] \n",
    "    \n",
    "    # dimension reduction method (default: PCA)\n",
    "    # 'PCA' or 'UMAP' or 'TSNE' or 'AdaptiveTSNE'\n",
    "    dim_method = 'PCA' \n",
    "    \n",
    "    # dimensions after reduction\n",
    "    n_components = 16\n",
    "    \n",
    "    # clustering algorithm (default: Kmeans)\n",
    "    # 'Kmeans' or 'AdaptiveKmeans' or 'PIC' or 'HDBscan' \n",
    "    clustering = 'Kmeans'\n",
    "\n",
    "    # number of cluster for k-means (default: 10000)\n",
    "    nmb_cluster = 104\n",
    "    \n",
    "    # parameter for adaptive k-means\n",
    "    end_k = 39\n",
    "    end_epochs = 150\n",
    "    \n",
    "    # parameter for PIC\n",
    "    sigma = 0.2 \n",
    "    nnn = 5\n",
    "    \n",
    "    # paramter for HDBscan\n",
    "    min_cluster_size = 12\n",
    "    min_samples = 3\n",
    "    nnn = 25\n",
    "    \n",
    "    # mini-batch size (default: 256)\n",
    "    batch = 16\n",
    "    \n",
    "    # learning rate (default: 0.05)\n",
    "    lr = 0.05\n",
    "    \n",
    "    # weight decay pow (default: -5)\n",
    "    wd=-5\n",
    "    \n",
    "    # number of total epochs to run (default: 200)\n",
    "    epochs = 50\n",
    "\n",
    "    # momentum (default: 0.9)\n",
    "    momentum = 0.9\n",
    "\n",
    "    # how many epochs of training between two consecutive reassignments of clusters (default: 1)\n",
    "    reassign = 1\n",
    "\n",
    "    # number of data loading workers (default: 4)\n",
    "    workers=4\n",
    "    \n",
    "    # cuda version\n",
    "    cuda = 10\n",
    "    \n",
    "    # Epoch to continue from (default: None)\n",
    "    resume = None\n",
    "    \n",
    "    # manual epoch number (useful on restarts) (default: 0)\n",
    "    start_epoch = 0\n",
    "    \n",
    "    # random seed (default: 42)\n",
    "    seed = 12\n",
    "    \n",
    "    # chatty\n",
    "    verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: EXP01_Kmeans_cl104_PCA-16d_COMBAT_TVN\n"
     ]
    }
   ],
   "source": [
    "exp_name = \"\"\n",
    "exp_name +='{}_'.format(args.run_ID)\n",
    "\n",
    "## Add clustering algorithm\n",
    "if args.clustering == \"Kmeans\":\n",
    "    clustering_parameter = 'Kmeans_cl{}'.format(args.nmb_cluster)\n",
    "elif args.clustering == \"AdaptiveKmeans\":\n",
    "    clustering_parameter = 'AdaptiveKmeans_cl{}-{}_ep{}'.format(args.nmb_cluster, args.end_k, args.end_epochs)\n",
    "elif args.clustering == \"PIC\":\n",
    "    clustering_parameter = 'PIC_sig{}_nn{}'.format(args.sigma, args.nnn)\n",
    "elif args.clustering == \"HDBscan\":\n",
    "    clustering_parameter = 'HDBscan_mincl{}-minsam{}_nn{}'.format(args.min_cluster_size, args.min_samples, args.nnn)\n",
    "\n",
    "exp_name +='{}'.format(clustering_parameter)\n",
    "    \n",
    "## Add dimensionality reduction method\n",
    "exp_name +='_{}-{}d'.format(args.dim_method, args.n_components)\n",
    "\n",
    "## Add batch correction\n",
    "batch_corrections_parameter = None\n",
    "if args.batch_corrections == None:\n",
    "    batch_corrections_parameter = \"NoCorrection\"\n",
    "else:\n",
    "    for corr in args.batch_corrections:\n",
    "        if batch_corrections_parameter is None:\n",
    "            batch_corrections_parameter = corr\n",
    "        else:\n",
    "            batch_corrections_parameter +='_{}'.format(corr)\n",
    "exp_name +='_{}'.format(batch_corrections_parameter)\n",
    "    \n",
    "print('Experiment: {}'.format(exp_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output path: /path/to/output/folder/EXP01_Kmeans_cl104_PCA-16d_COMBAT_TVN\n"
     ]
    }
   ],
   "source": [
    "args.output_path = os.path.join(args.save_path, exp_name)\n",
    "print('Output path: {}'.format(args.output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_path = os.path.join(args.output_path, 'y-embed_{}.txt'.format(exp_name))\n",
    "metadata_path = os.path.join(args.output_path, 'metadata_{}.txt'.format(exp_name))\n",
    "assign_epoch_path = os.path.join(args.output_path, 'epoch_assignment_{}.txt'.format(exp_name))\n",
    "metrices_path = os.path.join(args.output_path, 'metrices_{}.txt'.format(exp_name))\n",
    "loss_path = os.path.join(args.output_path, 'loss_{}.txt'.format(exp_name))\n",
    "\n",
    "best_labeled_sil_embed_path = os.path.join(args.output_path, 'best_labeled_sil_y-embed_{}.txt'.format(exp_name))\n",
    "best_labeled_nsc_embed_path = os.path.join(args.output_path, 'best_labeled_nsc_y-embed_{}.txt'.format(exp_name))\n",
    "best_unlabeled_embed_path = os.path.join(args.output_path, 'best_unlabeled_y-embed_{}.txt'.format(exp_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds_cols = ['Z{:03d}'.format(i) for i in range(args.n_features)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Multi-Scale Neural Network\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [27], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m model\u001b[39m.\u001b[39mtop_layer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     13\u001b[0m model\u001b[39m.\u001b[39mfeatures \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mDataParallel(model\u001b[39m.\u001b[39mfeatures)\n\u001b[1;32m---> 14\u001b[0m model\u001b[39m.\u001b[39;49mcuda()\n\u001b[0;32m     15\u001b[0m cudnn\u001b[39m.\u001b[39mbenchmark \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m# create optimizer\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marfok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:905\u001b[0m, in \u001b[0;36mModule.cuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    888\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[0;32m    889\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m    890\u001b[0m \n\u001b[0;32m    891\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    903\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[0;32m    904\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 905\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(\u001b[39mlambda\u001b[39;49;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
      "File \u001b[1;32mc:\\Users\\marfok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marfok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: Module._apply at line 797 (1 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\marfok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marfok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[0;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mc:\\Users\\marfok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:905\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    888\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[0;32m    889\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m    890\u001b[0m \n\u001b[0;32m    891\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    903\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[0;32m    904\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 905\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(\u001b[39mlambda\u001b[39;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
      "File \u001b[1;32mc:\\Users\\marfok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\cuda\\__init__.py:239\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    236\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    238\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 239\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    240\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    241\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m    242\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# fix random seeds\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "# CNN\n",
    "if args.verbose:\n",
    "    print('Create Multi-Scale Neural Network')\n",
    "    \n",
    "model = MultiScaleNet(input_dim=3, num_features=args.n_features, num_classes=args.nmb_cluster)\n",
    "fd = int(model.top_layer.weight.size()[1])\n",
    "model.top_layer = None\n",
    "model.features = torch.nn.DataParallel(model.features)\n",
    "model.cuda()\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# create optimizer\n",
    "optimizer = torch.optim.SGD(\n",
    "    filter(lambda x: x.requires_grad, model.parameters()),\n",
    "    lr=args.lr,\n",
    "    momentum=args.momentum,\n",
    "    weight_decay=10**args.wd,\n",
    ")\n",
    "\n",
    "# define loss function\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "# optionally resume from a checkpoint\n",
    "if args.resume:\n",
    "    checkpoint_path = os.path.join(args.output_path, 'checkpoints', 'checkpoint_epoch{}.pth.tar'.format(args.resume))\n",
    "    if os.path.isfile(checkpoint_path):\n",
    "        print(\"=> loading checkpoint '{}'\".format(checkpoint_path))\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        args.start_epoch = checkpoint['epoch']\n",
    "        # remove top_layer parameters from checkpoint\n",
    "        for key in list(checkpoint['state_dict']):\n",
    "            if 'top_layer' in key:\n",
    "                del checkpoint['state_dict'][key]\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "              .format(args.resume, checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
    "\n",
    "# creating checkpoint repo\n",
    "exp_check = os.path.join(args.output_path, 'checkpoints')\n",
    "if not os.path.isdir(exp_check):\n",
    "    os.makedirs(exp_check)\n",
    "\n",
    "# creating cluster assignments log\n",
    "cluster_log = Logger(os.path.join(args.output_path, 'train_data.pkl'))\n",
    "if args.resume:\n",
    "    cluster_log.load_data(args.resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [6, 3, 3, 3], expected input[2, 2, 512, 640] to have 3 channels, but got 2 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [29], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mverbose:\n\u001b[1;32m----> 2\u001b[0m     summary(model, input_size\u001b[39m=\u001b[39;49m(\u001b[39m2\u001b[39;49m, \u001b[39m512\u001b[39;49m, \u001b[39m640\u001b[39;49m))\n",
      "File \u001b[1;32mc:\\Users\\marfok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchsummary\\torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[1;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[0;32m     68\u001b[0m model\u001b[39m.\u001b[39mapply(register_hook)\n\u001b[0;32m     70\u001b[0m \u001b[39m# make a forward pass\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m# print(x.shape)\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m model(\u001b[39m*\u001b[39;49mx)\n\u001b[0;32m     74\u001b[0m \u001b[39m# remove these hooks\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m hooks:\n",
      "File \u001b[1;32mc:\\Users\\marfok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\marfok\\OneDrive - Bristol Myers Squibb\\Desktop\\Archetecture 2.0\\DSAA\\HCS\\Unsupervised\\UMM-Discovery_repo\\UMM-Discovery\\UMM-Discovery\\model.py:32\u001b[0m, in \u001b[0;36mMultiScaleNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 32\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures(x)\n\u001b[0;32m     33\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     34\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(x)\n",
      "File \u001b[1;32mc:\\Users\\marfok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1535\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1536\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1538\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1539\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1540\u001b[0m     \u001b[39mfor\u001b[39;00m hook_id, hook \u001b[39min\u001b[39;00m (\n\u001b[0;32m   1541\u001b[0m         \u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[0;32m   1542\u001b[0m         \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[0;32m   1543\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\marfok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py:153\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(\u001b[39m\"\u001b[39m\u001b[39mDataParallel.forward\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    152\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids:\n\u001b[1;32m--> 153\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule(\u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    155\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m chain(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule\u001b[39m.\u001b[39mparameters(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule\u001b[39m.\u001b[39mbuffers()):\n\u001b[0;32m    156\u001b[0m         \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mdevice \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msrc_device_obj:\n",
      "File \u001b[1;32mc:\\Users\\marfok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\marfok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\marfok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1535\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1536\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1538\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1539\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1540\u001b[0m     \u001b[39mfor\u001b[39;00m hook_id, hook \u001b[39min\u001b[39;00m (\n\u001b[0;32m   1541\u001b[0m         \u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[0;32m   1542\u001b[0m         \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[0;32m   1543\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\marfok\\OneDrive - Bristol Myers Squibb\\Desktop\\Archetecture 2.0\\DSAA\\HCS\\Unsupervised\\UMM-Discovery_repo\\UMM-Discovery\\UMM-Discovery\\model.py:90\u001b[0m, in \u001b[0;36mMSBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     89\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool1(x)\n\u001b[1;32m---> 90\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[0;32m     91\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(out)\n\u001b[0;32m     92\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu1(out)\n",
      "File \u001b[1;32mc:\\Users\\marfok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1535\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1536\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1538\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1539\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1540\u001b[0m     \u001b[39mfor\u001b[39;00m hook_id, hook \u001b[39min\u001b[39;00m (\n\u001b[0;32m   1541\u001b[0m         \u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[0;32m   1542\u001b[0m         \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[0;32m   1543\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\marfok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\marfok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [6, 3, 3, 3], expected input[2, 2, 512, 640] to have 3 channels, but got 2 channels instead"
     ]
    }
   ],
   "source": [
    "if args.verbose:\n",
    "    summary(model, input_size=(2, 512, 640))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.cuda==10:\n",
    "    # create tensorboard writer\n",
    "    tensorboard_path = os.path.join(args.save_path, 'runs', exp_name)\n",
    "\n",
    "    if os.path.isdir(tensorboard_path) and not args.resume:\n",
    "        shutil.rmtree(tensorboard_path)\n",
    "\n",
    "    if not os.path.isdir(tensorboard_path):\n",
    "        os.makedirs(tensorboard_path)\n",
    "    \n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    tensorboard_writer = SummaryWriter(log_dir=tensorboard_path)\n",
    "else:\n",
    "    tensorboard_writer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\marfok\\\\OneDrive - Bristol Myers Squibb\\\\Desktop\\\\Archetecture 2.0\\\\DSAA\\\\HCS\\\\Unsupervised\\\\UMM-Discovery_repo\\\\UMM-Discovery\\\\UMM-Discovery'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample_data.csv'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'compound'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\marfok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\marfok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\marfok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'compound'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39m# load the data\u001b[39;00m\n\u001b[0;32m      4\u001b[0m end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m----> 6\u001b[0m dataset \u001b[39m=\u001b[39m bbbc021_dataset(args\u001b[39m.\u001b[39;49mdata, args\u001b[39m.\u001b[39;49mcsv, \u001b[39m'\u001b[39;49m\u001b[39mpseudoclass\u001b[39;49m\u001b[39m'\u001b[39;49m, transform\u001b[39m=\u001b[39;49mtransforms\u001b[39m.\u001b[39;49mCompose(tra))\n\u001b[0;32m      8\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mverbose: \n\u001b[0;32m      9\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mLoad dataset: \u001b[39m\u001b[39m{0:.2f}\u001b[39;00m\u001b[39m s\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m end))\n",
      "File \u001b[1;32mc:\\Users\\marfok\\OneDrive - Bristol Myers Squibb\\Desktop\\Archetecture 2.0\\DSAA\\HCS\\Unsupervised\\UMM-Discovery_repo\\UMM-Discovery\\UMM-Discovery\\my_dataset.py:33\u001b[0m, in \u001b[0;36mbbbc021_dataset.__init__\u001b[1;34m(self, root_dir, metadata, label_header, transform)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39m=\u001b[39m transform\n\u001b[0;32m     31\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_header \u001b[39m=\u001b[39m label_header \u001b[39m#column in the dataset for unique classes?\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_columns()\n\u001b[0;32m     35\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_to_idx \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m() \u001b[39m# creates an empty dictionary\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses \u001b[39m=\u001b[39m [] \u001b[39m# creates an empty list\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marfok\\OneDrive - Bristol Myers Squibb\\Desktop\\Archetecture 2.0\\DSAA\\HCS\\Unsupervised\\UMM-Discovery_repo\\UMM-Discovery\\UMM-Discovery\\my_dataset.py:78\u001b[0m, in \u001b[0;36mbbbc021_dataset.add_columns\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     76\u001b[0m rel_dose_list \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m()\n\u001b[0;32m     77\u001b[0m \u001b[39mfor\u001b[39;00m i, row \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf\u001b[39m.\u001b[39miterrows():\n\u001b[1;32m---> 78\u001b[0m     compound \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdf\u001b[39m.\u001b[39;49mloc[i, \u001b[39m'\u001b[39;49m\u001b[39mcompound\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[0;32m     79\u001b[0m     \u001b[39mif\u001b[39;00m (compound \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mDMSO\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     80\u001b[0m         rel_dose_list\u001b[39m.\u001b[39mappend(\u001b[39m0\u001b[39m)\u001b[39m#\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marfok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1066\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1064\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(com\u001b[39m.\u001b[39mapply_if_callable(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m key)\n\u001b[0;32m   1065\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m-> 1066\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49m_get_value(\u001b[39m*\u001b[39;49mkey, takeable\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_takeable)\n\u001b[0;32m   1067\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_tuple(key)\n\u001b[0;32m   1068\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1069\u001b[0m     \u001b[39m# we by definition only have the 0th axis\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marfok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:3914\u001b[0m, in \u001b[0;36mDataFrame._get_value\u001b[1;34m(self, index, col, takeable)\u001b[0m\n\u001b[0;32m   3911\u001b[0m     series \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ixs(col, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m   3912\u001b[0m     \u001b[39mreturn\u001b[39;00m series\u001b[39m.\u001b[39m_values[index]\n\u001b[1;32m-> 3914\u001b[0m series \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_item_cache(col)\n\u001b[0;32m   3915\u001b[0m engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_engine\n\u001b[0;32m   3917\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, MultiIndex):\n\u001b[0;32m   3918\u001b[0m     \u001b[39m# CategoricalIndex: Trying to use the engine fastpath may give incorrect\u001b[39;00m\n\u001b[0;32m   3919\u001b[0m     \u001b[39m#  results if our categories are integers that dont match our codes\u001b[39;00m\n\u001b[0;32m   3920\u001b[0m     \u001b[39m# IntervalIndex: IntervalTree has no get_loc\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marfok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:4271\u001b[0m, in \u001b[0;36mDataFrame._get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   4266\u001b[0m res \u001b[39m=\u001b[39m cache\u001b[39m.\u001b[39mget(item)\n\u001b[0;32m   4267\u001b[0m \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   4268\u001b[0m     \u001b[39m# All places that call _get_item_cache have unique columns,\u001b[39;00m\n\u001b[0;32m   4269\u001b[0m     \u001b[39m#  pending resolution of GH#33047\u001b[39;00m\n\u001b[1;32m-> 4271\u001b[0m     loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(item)\n\u001b[0;32m   4272\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ixs(loc, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m   4274\u001b[0m     cache[item] \u001b[39m=\u001b[39m res\n",
      "File \u001b[1;32mc:\\Users\\marfok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3806\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3807\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3810\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'compound'"
     ]
    }
   ],
   "source": [
    "tra = [transforms.ToTensor()]\n",
    "    \n",
    "# load the data\n",
    "end = time.time()\n",
    "\n",
    "dataset = bbbc021_dataset(args.data, args.csv, 'pseudoclass', transform=transforms.Compose(tra))\n",
    "\n",
    "if args.verbose: \n",
    "    print('Load dataset: {0:.2f} s'.format(time.time() - end))\n",
    "    \n",
    "dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                         batch_size=args.batch,\n",
    "                                         num_workers=args.workers,\n",
    "                                         pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.get_df().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of compounds: {}'.format(len(dataset.get_df()['compound'].unique())))\n",
    "print('Number of treatments: {}'.format(len(dataset.get_df()['pseudoclass'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.get_df().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    img = dataset[i][0]\n",
    "\n",
    "    plt.figure(figsize=(20,10))\n",
    "    ax = plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img[0,:,:])\n",
    "    \n",
    "    ax = plt.subplot(1, 3, 2)\n",
    "    plt.imshow(img[1,:,:])\n",
    "    \n",
    "    ax = plt.subplot(1, 3, 3)\n",
    "    plt.imshow(img[2,:,:])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup clustering algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering algorithm to use\n",
    "if args.clustering == \"Kmeans\":\n",
    "    deepcluster = clustering.Kmeans(k=args.nmb_cluster, \n",
    "                                    dim_method=args.dim_method, \n",
    "                                    n_components=args.n_components,\n",
    "                                    n_jobs=args.workers+1)\n",
    "elif args.clustering == \"AdaptiveKmeans\":\n",
    "    deepcluster = clustering.AdaptiveKmeans(initial_k=args.nmb_cluster, \n",
    "                                            end_k=args.end_k, \n",
    "                                            end_epochs=args.end_epochs, \n",
    "                                            initial_epoch=args.start_epoch, \n",
    "                                            dim_method=args.dim_method, \n",
    "                                            n_components=args.n_components,\n",
    "                                            n_jobs=args.workers+1)\n",
    "elif args.clustering == \"PIC\":\n",
    "    deepcluster = clustering.PIC(sigma=args.sigma, \n",
    "                                 nnn=args.nnn, \n",
    "                                 dim_method=args.dim_method, \n",
    "                                 n_components=args.n_components,\n",
    "                                 n_jobs=args.workers+1)\n",
    "elif args.clustering == \"HDBscan\":\n",
    "    deepcluster = clustering.HDBscan(min_cluster_size=args.min_cluster_size, \n",
    "                                    min_samples=args.min_samples, \n",
    "                                    nnn=args.nnn, \n",
    "                                    dim_method=args.dim_method, \n",
    "                                    n_components=args.n_components,\n",
    "                                    n_jobs=args.workers+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "nmi_list = [0]\n",
    "best_NSC_acc = 0\n",
    "df_meta = dataset.get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training convnet with DeepCluster\n",
    "start_train_time = time.time()\n",
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "    end = time.time()\n",
    "\n",
    "    # remove head\n",
    "    model.top_layer = None\n",
    "    model.classifier = nn.Sequential(*list(model.classifier.children())[:-1])\n",
    "\n",
    "    # compute features for the whole dataset\n",
    "    features = compute_features(dataloader, model, len(dataset), args)\n",
    "    #print(features.shape)\n",
    "    embeds = pd.DataFrame(features, columns=embeds_cols)\n",
    "    embeds = pd.concat([df_meta, embeds], axis=1)\n",
    "    \n",
    "    # batch correction \n",
    "    embeds = do_batch_correction(embeds, embeds_cols, args.batch_corrections, verbose=args.verbose)\n",
    "        \n",
    "    # evaluate features\n",
    "    df_eval = evaluate_epoch(embeds.copy(), embeds_cols, verbose=args.verbose)\n",
    "    \n",
    "    # best epoch\n",
    "    NSC_acc = df_eval[\"NSC_1-NN_treatment\"][0]\n",
    "    if NSC_acc > best_NSC_acc:\n",
    "        best_NSC_acc = NSC_acc\n",
    "    \n",
    "    # cluster the features\n",
    "    clustering_loss = deepcluster.cluster(embeds[embeds_cols].to_numpy(copy=True), verbose=args.verbose)\n",
    "\n",
    "    # save cluster assignment\n",
    "    df_assign_epoch = pd.DataFrame(clustering.arrange_clustering(deepcluster.images_lists), columns=['Assignment_{}'.format(epoch)])\n",
    "\n",
    "    # assign pseudo-labels\n",
    "    train_dataset = clustering.cluster_assign(deepcluster.images_lists, dataset.imgs)\n",
    "\n",
    "    # uniformely sample per target\n",
    "    sampler = UnifLabelSampler(int(args.reassign * len(train_dataset)),\n",
    "                               deepcluster.images_lists)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch,\n",
    "        num_workers=args.workers,\n",
    "        sampler=sampler,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    # set last fully connected layer\n",
    "    mlp = list(model.classifier.children())\n",
    "    mlp.append(nn.ReLU(inplace=True).cuda())\n",
    "    model.classifier = nn.Sequential(*mlp)\n",
    "    model.top_layer = nn.Linear(args.n_features, len(deepcluster.images_lists))\n",
    "    model.top_layer.weight.data.normal_(0, 0.01)\n",
    "    model.top_layer.bias.data.zero_()\n",
    "    model.top_layer.cuda()\n",
    "\n",
    "    # train network with clusters as pseudo-labels\n",
    "    start_epoch_train_time = time.time()\n",
    "    loss = train(train_dataloader, model, criterion, optimizer, epoch, args)\n",
    "    loss_list.append(float(loss))\n",
    "\n",
    "    # print log\n",
    "    if args.verbose:\n",
    "        print('###### Epoch [{0}] ###### \\n'\n",
    "              'Total Time: {1:.3f} s\\n'\n",
    "              'Training time: {2:.3f} s\\n'\n",
    "              'ConvNet loss: {3:.3f} \\n'\n",
    "              'NSC acc: {4:.3f} \\n'\n",
    "              'Silhoutte score: {5:.3f} \\n'\n",
    "              'Best NSC acc: {6:.3f}'\n",
    "              .format(epoch, \n",
    "                      time.time() - end, \n",
    "                      time.time() - start_epoch_train_time, \n",
    "                      loss, \n",
    "                      df_eval[\"NSC_1-NN_treatment\"][0]*100, \n",
    "                      df_eval[\"silhou_moa\"][0],\n",
    "                      best_NSC_acc*100\n",
    "                     ))\n",
    "        try:\n",
    "            if epoch > 0:\n",
    "                nmi = normalized_mutual_info_score(\n",
    "                    clustering.arrange_clustering(deepcluster.images_lists),\n",
    "                    clustering.arrange_clustering(cluster_log.data['clustering'][-1])\n",
    "                )\n",
    "                print('NMI against previous assignment: {0:.3f}'.format(nmi))\n",
    "            else:\n",
    "                nmi = 0\n",
    "            nmi_list.append(nmi)\n",
    "        except IndexError:\n",
    "            pass\n",
    "        print('####################### \\n')\n",
    "    # save running checkpoint\n",
    "    torch.save({'epoch': epoch + 1,\n",
    "                'arch': args.arch,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer' : optimizer.state_dict()},\n",
    "               os.path.join(args.output_path, 'checkpoint.pth.tar'))\n",
    "\n",
    "    # save cluster assignments, evaluation data and features\n",
    "    cluster_log.log(epoch, embeds, df_eval, df_assign_epoch, loss, nmi, deepcluster.images_lists)\n",
    "    \n",
    "    # write to tensorboard\n",
    "    if tensorboard_writer is not None:\n",
    "        tensorboard_writer.add_scalar('loss', loss, epoch)\n",
    "        tensorboard_writer.add_scalar('nmi', nmi, epoch)\n",
    "        for eval_metric, data in df_eval.iteritems():\n",
    "            tensorboard_writer.add_scalar(eval_metric, data[0], epoch)\n",
    "        \n",
    "print('Total training time {}'.format(time.time() - start_train_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate training"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# load training data (for loading a completed training)\n",
    "cluster_log.load_data(args.resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_log.data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_log.data['features'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metric = pd.concat(cluster_log.data['eval_metrices'], axis=0)\n",
    "df_metric[\"epoch\"] = cluster_log.data['epoch']\n",
    "df_metric[\"loss\"] = cluster_log.data['loss']\n",
    "df_metric[\"loss\"] = df_metric[\"loss\"].map(float)\n",
    "df_metric[\"nmi\"] = cluster_log.data['nmi']\n",
    "df_metric[\"nmi\"] = df_metric[\"nmi\"].map(float)\n",
    "df_metric = df_metric.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metric = df_metric.rename(columns={'n_clusters_well':'n_clusters'})\n",
    "df_metric['total_score'] = (df_metric['adj-mut_comp_pred'] + df_metric['comple_treat-pred'] + df_metric['silhou_pred_tsne'])/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_metric.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(df_metric, args.output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_summary(df_metric, args.output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal partitional validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_inter_part_validation(df_metric, args.output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal hierarchical validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_inter_hier_validation(df_metric, args.output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_external_validation(df_metric, args.output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_NSC_NSB(df_metric, args.output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_total_score(df_metric, args.output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best epoch for labeled and unlabeled situation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best epoch labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch_labeled_silh = df_metric['silhou_moa_tsne'].idxmax()\n",
    "print('Best epoch (Silhouette): ' , best_epoch_labeled_silh)\n",
    "print('NSC: ', df_metric[\"NSC_1-NN_treatment\"][best_epoch_labeled_silh])\n",
    "print('Silhouette moa tsne: ', df_metric['silhou_moa_tsne'][best_epoch_labeled_silh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch_labeled_nsc = df_metric['NSC_1-NN_treatment'].idxmax()\n",
    "print('Best epoch (NSC): ' , best_epoch_labeled_nsc)\n",
    "print('NSC: ', df_metric[\"NSC_1-NN_treatment\"][best_epoch_labeled_nsc])\n",
    "print('Silhouette moa tsne: ', df_metric['silhou_moa_tsne'][best_epoch_labeled_nsc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best epoch unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch_unlabeled = df_metric.loc[:,'total_score'].idxmax()\n",
    "print('Best epoch (unlabeled): ' , best_epoch_unlabeled)\n",
    "print('NSC: ', df_metric['NSC_1-NN_treatment'][best_epoch_unlabeled])\n",
    "print('Silhouette pred tsne: ', df_metric['silhou_pred_tsne'][best_epoch_unlabeled])\n",
    "print('Adjusted Rand index (treat-pred): ', df_metric['adj-rand_treat-pred'][best_epoch_unlabeled])\n",
    "print('Adjusted Mutual information (treat-pred): ', df_metric['adj-mut_treat-pred'][best_epoch_unlabeled])\n",
    "print('Adjusted Mutual information (moa-pred): ', df_metric['adj-mut_moa-pred'][best_epoch_unlabeled])\n",
    "print('Jaccard (treat-pred): ', df_metric['jaccard_treat-pred'][best_epoch_unlabeled])\n",
    "print('Completeness (treat-pred): ', df_metric['comple_treat-pred'][best_epoch_unlabeled])\n",
    "print('Number of clusters: ', df_metric['n_clusters'][best_epoch_unlabeled])\n",
    "print('Total score: ', df_metric['total_score'][best_epoch_unlabeled])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save hyperparameter with best values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tensorboard_writer is not None:\n",
    "    tensorboard_writer.add_hparams(hparam_dict={\n",
    "                                    'cuda_version': args.cuda,\n",
    "                                    'lr': args.lr, \n",
    "                                    'batch_size': args.batch, \n",
    "                                    'n_features': args.n_features,\n",
    "                                    'batch_correction': batch_corrections_parameter,\n",
    "                                    'dim_method': args.dim_method, \n",
    "                                    'n_features_reduced': args.n_components,\n",
    "                                    'clustering_algorithmn': args.clustering,\n",
    "                                    'clustering_parameter': clustering_parameter,\n",
    "        \n",
    "                                   }, \n",
    "                                   metric_dict={\n",
    "                                       # labeled best silhoutte score moa\n",
    "                                       'labeled_best_silh/epoch': best_epoch_labeled_silh,\n",
    "                                       'labeled_best_silh/silhou_moa_tsne': df_metric['silhou_moa_tsne'][best_epoch_labeled_silh],\n",
    "                                       'labeled_best_silh/NSC_1-NN_treatment': df_metric['NSC_1-NN_treatment'][best_epoch_labeled_silh],\n",
    "                                       'labeled_best_silh/NSC_1-NN_avg': df_metric['NSC_1-NN_avg'][best_epoch_labeled_silh],\n",
    "                                       # labeled NSC\n",
    "                                       'labeled_best_nsc/epoch': best_epoch_labeled_nsc,\n",
    "                                       'labeled_best_nsc/silhou_moa_tsne': df_metric['silhou_moa_tsne'][best_epoch_labeled_nsc],\n",
    "                                       'labeled_best_nsc/NSC_1-NN_treatment': df_metric['NSC_1-NN_treatment'][best_epoch_labeled_nsc],\n",
    "                                       'labeled_best_nsc/NSC_1-NN_avg': df_metric['NSC_1-NN_avg'][best_epoch_labeled_nsc],\n",
    "                                       # unlabeled\n",
    "                                       'unlabeled/epoch': best_epoch_unlabeled,\n",
    "                                       'unlabeled/NSC_1-NN_treatment': df_metric['NSC_1-NN_treatment'][best_epoch_unlabeled],\n",
    "                                       'unlabeled/silhou_moa_tsne': df_metric['silhou_moa_tsne'][best_epoch_unlabeled],\n",
    "                                       'unlabeled/silhou_pred_tsne': df_metric['silhou_pred_tsne'][best_epoch_unlabeled],\n",
    "                                       'unlabeled/adj-rand_treat-pred': df_metric['adj-rand_treat-pred'][best_epoch_unlabeled],\n",
    "                                       'unlabeled/adj-mut_treat-pred': df_metric['adj-mut_treat-pred'][best_epoch_unlabeled],\n",
    "                                       'unlabeled/jaccard_treat-pred': df_metric['jaccard_treat-pred'][best_epoch_unlabeled],\n",
    "                                       'unlabeled/comple_treat-pred': df_metric['comple_treat-pred'][best_epoch_unlabeled],\n",
    "                                       'unlabeled/n_clusters': df_metric['n_clusters'][best_epoch_unlabeled],\n",
    "                                       'unlabeled/total_score': df_metric['total_score'][best_epoch_unlabeled]\n",
    "                                   })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metric.to_csv(metrices_path, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = dataset.get_df()\n",
    "df_meta.to_csv(metadata_path, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save cluster assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_list = cluster_log.data['cluster_assignments']\n",
    "df_assignments = pd.concat(assign_list, axis=1)\n",
    "df_assignments = pd.concat([df_meta, df_assignments], axis=1)\n",
    "df_assignments.to_csv(assign_epoch_path, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df = pd.DataFrame({'model_loss':cluster_log.data['loss'], \n",
    "                        'NMI':cluster_log.data['nmi']})\n",
    "loss_df.to_csv(loss_path, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the best embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_labeled_sil_embeds = cluster_log.data['features'][best_epoch_labeled_silh]\n",
    "df_best_labeled_nsc_embeds = cluster_log.data['features'][best_epoch_labeled_nsc]\n",
    "df_best_unlabeled_embeds = cluster_log.data['features'][best_epoch_unlabeled]\n",
    "\n",
    "df_best_labeled_sil_embeds.to_csv(best_labeled_sil_embed_path, sep='\\t', index=False)\n",
    "df_best_labeled_nsc_embeds.to_csv(best_labeled_nsc_embed_path, sep='\\t', index=False)\n",
    "df_best_unlabeled_embeds.to_csv(best_unlabeled_embed_path, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best epoch evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_best_labeled_sil_save_well = evaluate_training(df_best_labeled_sil_embeds, embeds_cols, os.path.join(args.output_path,'best_labeled_sil'), verbose=args.verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_best_labeled_ncs_save_well = evaluate_training(df_best_labeled_nsc_embeds, embeds_cols, os.path.join(args.output_path,'best_labeled_nsc'), verbose=args.verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_best_unlabeled_save_well = evaluate_training(df_best_unlabeled_embeds, embeds_cols, os.path.join(args.output_path,'best_unlabeled'), verbose=args.verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "03563d318b1a3875a69394994e2085503605e202bab791be76df3a041f0288fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
